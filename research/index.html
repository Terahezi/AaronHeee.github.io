---
layout: default
title: Project | Zi He
home: passive
working: passive
research: active
Cproject: passive
Iproject: passive
life: passive
blog: passive
description: Research
---
<div class="cell">
    <table>
        <tr>
            <td>
                <a class="title" ><b>Sequential Recommendation</b></a><br/> This is a research I have currently been doing on Recommender System. Evaluated on Hit@k, NDCG@k, AUC. Written in Python, Tensorflow. <br/> 
                <li>Implemented Factorized Personalized Markov Chain and Translation-based Recommender on a Steam reviews dataset for sequential recommendation, compared with baseline model: Bayesian Personalized Ranking.</li>
                <li>Implemented Self-attentive Deep Neural Networks using ideas from Transformer on Steam reviews, Amazon Beauty/Games, MovieLens dataset for sequential recommendation.</li>
                <li>Introduce hidden variable to capture heterogeneous relationships among items, introduce multi-task learning for training.</li>
                <li>Introduce knowledge graph based context to item embeddings to capture item relations under sequential setting.</li>
            </td>
        </tr>
    </table>
</div>


<div class="cell">
    <table>
        <tr>
            <td>
                <a class="title" ><b>Visual Question Answering & Image Captioning</b></a><br/> Reimplemented and Improved VQA 2017 & 18 champion's work. Written in Pytorch <br/>
                <li>Implemented a baseline with simple CNN and RNN for feature extraction, element-wise summation for fusion and fully-connected neural networks for classification.</li>
                <li>Image captioning: Used faster-RCNN to extract bottom-up attention, used LSTM to extract top-down attention and generate output words.</li>
                <li>VQA17: Used faster-RCNN to extract bottom-up attention, used GRU to encode word embeddings of questions and generate top-down attention weights, applied multi-modal fusion and computed predicted scores.</li>
                <li>VQA18: Used ReLU after weight normalization instead of tanh for activation, replaced feature concatenation with element-wise multiplication, fine-tuned bottom-up attention, used Adamax for optimization.</li>
                <li>Applied dropout & gradient clipping, increased the number of hidden units in the network, applied projection-based attention, experimented with different types of fusion methods: block-based, concatenated MLP, MCB, MUTAN, MFH.</li>
                <small><u><a href="https://github.com/Terahezi/Visual-Question-Answering/blob/master/report.pdf">[PDF]</a></u>, <u><a href="https://github.com/Terahezi/Visual-Question-Answering">[CODE]</a></u></small> <br/> 
            </td>
            <td><img class="side" src="resource/5.png" width="160px" height="120px"></td>
        </tr>
    </table>
</div>
